{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's say we have to following problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Print the numbers 1 to 100, excepts that if the number is divisible by 3, print \"fizz\"; if the number is divisible by 5, print \"buzz\"; and if the number is divisible by 15, print \"fizzbuzz\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.algebra import Vector, dot\n",
    "\n",
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1] # When 'x' is divisible by 15 (fizzbuzz)\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0] # When 'x' is divisible by 5 (buzz)\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0] # When 'x' is divisble by 3 (fizz)\n",
    "    else:\n",
    "        return [1, 0, 0, 0] # When 'x' is divisble by neither of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll use this to generate our target vectors. The input vectors are less obvious. You don't want to just use a one-dimensional vector containing the input number, for a couple of reasons. A single input captures an \"intensity\", but the fact that 2 is twice as much as 1, and that 4 is twice as much again, doesn't feel relevant to this problem. Additionally, with just one input the hidden layer wouldn't be able to compute very interesting features, which means it probably wouln't be able to solve the problem. It turns out that one thing that works reasonably well is to convert each number to its *binary* representation of 1s and 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Floor Division(\"//\"): The division of operands where the result is the quotient in which the digits after the decimal point are removed. But if one of the operands is negative, the result is floored , i.e., rounded away from zero (towards negative infinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulus(\"%\"): returns the remainder when the first operand is divided by the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "\n",
    "    return binary\n",
    "\n",
    "print(binary_encode(999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the goal is to construct the outputs for the numbers 1 to 100, it would be cheating to train on those numbers. Therefore, we'll train on the numbers 101 to 1,023 (which is the largest number we can represent with 10 binary digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our neural network will have 10 input neurons (since we're representing our inputs as 10-dimensional vector) and 4 output neurons (since we're representing our targets as 4-dimensional vectors). We'll give it 25 hidden units, but we'll use a variable for that so it's easy to change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [\n",
    "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)], # random weights\n",
    "\n",
    "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First lets import our previously defined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, imputs includes a 1\n",
    "    return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]], input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]\n",
    "        output = [neuron_output(neuron, input_with_bias) for neuron in layer]\n",
    "        outputs.append(output)\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz (loss: 179.85): 100%|██████████| 20000/20000 [1:46:18<00:00,  3.14it/s]     \n"
     ]
    }
   ],
   "source": [
    "from ml.algebra import squared_distance\n",
    "from ml.gradient_descent import gradient_step\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(20000) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            epoch_loss += squared_distance(predicted, y)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate) for neuron, grad in zip(layer, layer_grad)] for layer, layer_grad in zip(network, gradients)]\n",
    "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have one remaining issue. Our network will produce a 4-dimensional vector of numbers, but want a single prediction. we'll do that by taking the `argmax`, which is the index of the largest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs: list) -> int:\n",
    "    \"\"\" Returns the index of the largest value \"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "1 / 100\n",
      "2 2 2\n",
      "2 / 100\n",
      "3 fizz fizz\n",
      "3 / 100\n",
      "4 4 4\n",
      "4 / 100\n",
      "5 buzz buzz\n",
      "5 / 100\n",
      "6 6 fizz\n",
      "5 / 100\n",
      "7 7 7\n",
      "6 / 100\n",
      "8 8 8\n",
      "7 / 100\n",
      "9 fizz fizz\n",
      "8 / 100\n",
      "10 buzz buzz\n",
      "9 / 100\n",
      "11 fizz 11\n",
      "9 / 100\n",
      "12 12 fizz\n",
      "9 / 100\n",
      "13 13 13\n",
      "10 / 100\n",
      "14 14 14\n",
      "11 / 100\n",
      "15 fizzbuzz fizzbuzz\n",
      "12 / 100\n",
      "16 16 16\n",
      "13 / 100\n",
      "17 17 17\n",
      "14 / 100\n",
      "18 fizz fizz\n",
      "15 / 100\n",
      "19 19 19\n",
      "16 / 100\n",
      "20 buzz buzz\n",
      "17 / 100\n",
      "21 21 fizz\n",
      "17 / 100\n",
      "22 22 22\n",
      "18 / 100\n",
      "23 23 23\n",
      "19 / 100\n",
      "24 fizz fizz\n",
      "20 / 100\n",
      "25 buzz buzz\n",
      "21 / 100\n",
      "26 fizz 26\n",
      "21 / 100\n",
      "27 fizz fizz\n",
      "22 / 100\n",
      "28 28 28\n",
      "23 / 100\n",
      "29 29 29\n",
      "24 / 100\n",
      "30 fizzbuzz fizzbuzz\n",
      "25 / 100\n",
      "31 31 31\n",
      "26 / 100\n",
      "32 32 32\n",
      "27 / 100\n",
      "33 fizz fizz\n",
      "28 / 100\n",
      "34 34 34\n",
      "29 / 100\n",
      "35 35 buzz\n",
      "29 / 100\n",
      "36 fizz fizz\n",
      "30 / 100\n",
      "37 37 37\n",
      "31 / 100\n",
      "38 38 38\n",
      "32 / 100\n",
      "39 fizz fizz\n",
      "33 / 100\n",
      "40 buzz buzz\n",
      "34 / 100\n",
      "41 fizz 41\n",
      "34 / 100\n",
      "42 fizz fizz\n",
      "35 / 100\n",
      "43 43 43\n",
      "36 / 100\n",
      "44 44 44\n",
      "37 / 100\n",
      "45 fizz fizzbuzz\n",
      "37 / 100\n",
      "46 46 46\n",
      "38 / 100\n",
      "47 47 47\n",
      "39 / 100\n",
      "48 fizz fizz\n",
      "40 / 100\n",
      "49 49 49\n",
      "41 / 100\n",
      "50 50 buzz\n",
      "41 / 100\n",
      "51 fizz fizz\n",
      "42 / 100\n",
      "52 52 52\n",
      "43 / 100\n",
      "53 buzz 53\n",
      "43 / 100\n",
      "54 fizz fizz\n",
      "44 / 100\n",
      "55 55 buzz\n",
      "44 / 100\n",
      "56 fizz 56\n",
      "44 / 100\n",
      "57 fizz fizz\n",
      "45 / 100\n",
      "58 58 58\n",
      "46 / 100\n",
      "59 59 59\n",
      "47 / 100\n",
      "60 fizz fizzbuzz\n",
      "47 / 100\n",
      "61 61 61\n",
      "48 / 100\n",
      "62 62 62\n",
      "49 / 100\n",
      "63 63 fizz\n",
      "49 / 100\n",
      "64 64 64\n",
      "50 / 100\n",
      "65 buzz buzz\n",
      "51 / 100\n",
      "66 66 fizz\n",
      "51 / 100\n",
      "67 67 67\n",
      "52 / 100\n",
      "68 68 68\n",
      "53 / 100\n",
      "69 fizz fizz\n",
      "54 / 100\n",
      "70 buzz buzz\n",
      "55 / 100\n",
      "71 71 71\n",
      "56 / 100\n",
      "72 fizz fizz\n",
      "57 / 100\n",
      "73 73 73\n",
      "58 / 100\n",
      "74 74 74\n",
      "59 / 100\n",
      "75 fizzbuzz fizzbuzz\n",
      "60 / 100\n",
      "76 fizz 76\n",
      "60 / 100\n",
      "77 fizz 77\n",
      "60 / 100\n",
      "78 fizz fizz\n",
      "61 / 100\n",
      "79 79 79\n",
      "62 / 100\n",
      "80 buzz buzz\n",
      "63 / 100\n",
      "81 fizz fizz\n",
      "64 / 100\n",
      "82 82 82\n",
      "65 / 100\n",
      "83 83 83\n",
      "66 / 100\n",
      "84 fizz fizz\n",
      "67 / 100\n",
      "85 buzz buzz\n",
      "68 / 100\n",
      "86 86 86\n",
      "69 / 100\n",
      "87 87 fizz\n",
      "69 / 100\n",
      "88 88 88\n",
      "70 / 100\n",
      "89 89 89\n",
      "71 / 100\n",
      "90 fizzbuzz fizzbuzz\n",
      "72 / 100\n",
      "91 91 91\n",
      "73 / 100\n",
      "92 fizz 92\n",
      "73 / 100\n",
      "93 fizz fizz\n",
      "74 / 100\n",
      "94 94 94\n",
      "75 / 100\n",
      "95 95 buzz\n",
      "75 / 100\n",
      "96 fizz fizz\n",
      "76 / 100\n",
      "97 97 97\n",
      "77 / 100\n",
      "98 98 98\n",
      "78 / 100\n",
      "99 fizz fizz\n",
      "79 / 100\n",
      "100 buzz buzz\n",
      "80 / 100\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "\n",
    "for n in range(1, 101):\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "    print(n, labels[predicted], labels[actual])\n",
    "\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "    print(num_correct, \"/\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
