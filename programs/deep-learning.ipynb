{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tensor\n",
    "#### Previously, we made a distinction between vectors (one-dimensional arrays) and matrices (two-dimensional arrays). When we start working with more complicated neural networks, we'll need to use a higher-dimensional arrays as well.\n",
    "\n",
    "#### In many neural network libraries, *n*-dimensional arrays are referred to as *tensors*, which is what we'll call them too.\n",
    "#### If I were writing an entire book about deep learning, I'd implement a full-featured `Tensor` class that overloaded Python's arithmetic operators and could handle a variety of other operations. Such an implementation would take a notebook on its own. Here we'll cheat and say that a `Tensor` is just a `list`. This is true in one direction - all our vectors and matrices and higher-dimensional analogues *are* lists. It is certainly not true in the other direction - most Python *lists* are not *n*-dimensional arrays in our sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's write a helper function to find a tensor's *shape*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[3, 2]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "Tensor = list\n",
    "def shape(tensor: Tensor) -> List[int]:\n",
    "    sizes: List[int] = []\n",
    "    while isinstance(tensor, list):\n",
    "        sizes.append(len(tensor))\n",
    "        tensor = tensor[0]\n",
    "    return sizes\n",
    "\n",
    "print(shape([1, 2, 3]))\n",
    "print(shape([[1,2], [3,4], [5,6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because tensors can have any number of dimensions, we'll typically need to work with them recursively. We'll do one thing in the one-dimensional case and recurse in the higher-dimensional case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def is_1d(tensor: Tensor) -> bool:\n",
    "    \"\"\"\n",
    "    If tensor[0] is a list, it's a higher-order tensor. Otherwise, tensor is 1-dimensional (that is, a vector)\n",
    "    \"\"\"\n",
    "    return not isinstance(tensor[0], list)\n",
    "\n",
    "print(is_1d([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which we can use to write a recursive `tensor_sum` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "def tensor_sum(tensor: Tensor) -> float:\n",
    "    \"\"\" Sums up all the values in the tensor\"\"\"\n",
    "    if is_1d(tensor):\n",
    "        return sum(tensor)\n",
    "    else:\n",
    "        return sum(tensor_sum(tensor_i) for tensor_i in tensor)\n",
    "\n",
    "print(tensor_sum([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll create a couple of helper functions so that we don't have to rewrite this logic everywhere. The first applies a function elementwise to a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
    "    \"\"\" Applies f elementwise\"\"\" \n",
    "    if is_1d(tensor):\n",
    "        return[f(x) for x in tensor]\n",
    "    else:\n",
    "        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n",
    "\n",
    "print(tensor_apply(lambda x: x + 1, [1, 2, 3])) # So in this example, we are adding 1 to every instance of x - each tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use this to write a function that creates a zero tensor with the same shape as a given tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def zeros_like(tensor: Tensor) -> Tensor:\n",
    "    return tensor_apply(lambda _: 0.0, tensor)\n",
    "\n",
    "print(zeros_like([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We'll also need to apply a function to corresponding elements from two tensors (which had better be the exact same shape, although we won't check that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "[4, 10, 18]\n"
     ]
    }
   ],
   "source": [
    "def tensor_combine(f: Callable[[float, float], float],\n",
    "                    t1: Tensor,\n",
    "                    t2: Tensor) -> Tensor:\n",
    "    \"\"\" Applies f to corresponding elements of t1 and t2\"\"\"\n",
    "    if is_1d(t1):\n",
    "        return [f(x, y) for x,y in zip(t1, t2)]\n",
    "    else:\n",
    "        return [tensor_combine(f, t1_i, t2_i) for t1_i, t2_i in zip(t1, t2)]\n",
    "\n",
    "import operator\n",
    "print(tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]))\n",
    "print(tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Layer Abstraction \n",
    "#### In our previous notebook we built a simple neural net that allowed us to stack two layers of neurons, each of which computed `sigmoid(dot(weights, inputs))`.\n",
    "\n",
    "#### Although that's perhaps an idealized representation of what an actual neuron does, in practice we'd like to allow a wider variety of things. Perhaps we'd like the neurons to remember something about their previous inputs. Perhaps we'd like to use a different activation function than `sigmoid`. And frequently we'd like to use more than two layers. (Our `feed_forward` function actually handled any number of layers, but our gradient computations did not.)\n",
    "\n",
    "#### In this notebook we'll build machinery for implementing such a variety of neural networks. Our fundamental abstraction will be the `Layer`, something that knows how to apply some function to its inputs that knows how to backpropagate gradients.\n",
    "\n",
    "#### One way of thinking about the neural networks we built in `fizzbuzz.ipynb` is as a \"linear\" layer, followed by a \"sigmoid\" layer, then another linear layer and another sigmoid layer. We didn't distinguish them in these terms, but doing so will allow us to experiment with much more general structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Our neural networks will be composed of Layers, each of which \n",
    "    knows how to do some computation on its inputs in the \"forward\" \n",
    "    direction and propagate gradients in the \"backward\" direction\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Not the lack of typyes. We're not going to be prescriptive\n",
    "        about what kinds of inputs layer can take and what kinds of \n",
    "        outputs they can return.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Similarly, we're not going to be prescriptive about what the\n",
    "        gradient looks like. It's up to you the user to make sure \n",
    "        that you're doing things sensibly.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def params(self) -> Iterable[Tensor]:\n",
    "        \"\"\" \n",
    "        Returns the parameters of this layer. The default implementation\n",
    "        return nothing, so that if you have a layer with no parameters\n",
    "        you don't have to implement this.\n",
    "        \"\"\"\n",
    "        return ()\n",
    "    \n",
    "    def grads(self) -> Iterable[Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the gradients, in the same order as params()\n",
    "        \"\"\"\n",
    "        return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `forward` and `backward` methods will have to be implemented in our concrete subclasses. Once we build a neural net, we'll want to train it using gradient descent, which means we'll want to update each parameter in the network using its gradient. Accordingly, we'll insist that each layer be able to tell us its parameters and gradients\n",
    "\n",
    "#### Some layers (for example, a layer that applies `sigmoid` to each of its inputs) have no parameters to update, so we provide a default implementation that handles that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'linear_algebra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/aloksharma/Dropbox/Mac/Desktop/text-connonations/programs/deep-learning.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aloksharma/Dropbox/Mac/Desktop/text-connonations/programs/deep-learning.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneural_networks\u001b[39;00m \u001b[39mimport\u001b[39;00m sigmoid\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aloksharma/Dropbox/Mac/Desktop/text-connonations/programs/deep-learning.ipynb#ch0000016?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSigmoid\u001b[39;00m(Layer):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aloksharma/Dropbox/Mac/Desktop/text-connonations/programs/deep-learning.ipynb#ch0000016?line=3'>4</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "File \u001b[0;32m~/Dropbox/Mac/Desktop/text-connonations/programs/ml/neural_networks.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlinear_algebra\u001b[39;00m \u001b[39mimport\u001b[39;00m Vector, dot\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_function\u001b[39m(x: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1.0\u001b[39m \u001b[39mif\u001b[39;00m x \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0.0\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'linear_algebra'"
     ]
    }
   ],
   "source": [
    "from ml.neural_networks import sigmoid\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply sigmoid to each element of the input tensor,\n",
    "        and save the results to use in backpropagation\n",
    "        \"\"\"\n",
    "        self.sigmoids = tensor_apply(sigmoid, input)\n",
    "        return self.sigmoids\n",
    "        \n",
    "    def backward(self, gradient: Tensor) -> Tensor:\n",
    "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
    "                              self.sigmoids,\n",
    "                              gradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
